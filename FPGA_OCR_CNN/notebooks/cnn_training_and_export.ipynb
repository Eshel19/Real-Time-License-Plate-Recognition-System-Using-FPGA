{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a37502f",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# Parameters\n",
    "TRAIN_DIR = \"dataset_class_train\"\n",
    "VALI_DIR = \"dataset_class_valid\"\n",
    "WINDOW_HEIGHT = 18\n",
    "LEFT_PAD = 2\n",
    "BLANK_CLASS = 10\n",
    "MAX_WIDTH = 200\n",
    "\n",
    "def count_labels_across_strips(dataset):\n",
    "    label_counts = Counter()\n",
    "    for _, labels in dataset:\n",
    "        for label, _, _ in labels:\n",
    "            label_counts[label] += 1\n",
    "\n",
    "    print(\"\\nüîé Digit Counts in Generated Strips:\")\n",
    "    for k in sorted(label_counts):\n",
    "        name = str(k) if k != BLANK_CLASS else \"blank\"\n",
    "        print(f\"  Class {name}: {label_counts[k]}\")\n",
    "\n",
    "\n",
    "def pad_to_height(img_np, target_height=WINDOW_HEIGHT):\n",
    "    h, w = img_np.shape\n",
    "    pad_top = (target_height - h) // 2\n",
    "    pad_bottom = target_height - h - pad_top\n",
    "    return np.pad(img_np, ((pad_top, pad_bottom), (0, 0)), constant_values=255)\n",
    "\n",
    "\n",
    "def load_and_trim_digit_images(base_dir):\n",
    "    dataset = []\n",
    "    for cls in tqdm([str(i) for i in range(10)] + ['none'], desc=f\"Loading {base_dir}\"):\n",
    "        cls_path = os.path.join(base_dir, cls)\n",
    "        if not os.path.isdir(cls_path):\n",
    "            continue\n",
    "        for fname in os.listdir(cls_path):\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img = Image.open(os.path.join(cls_path, fname)).convert(\"L\")\n",
    "                img_np = np.array(img)\n",
    "                cols = np.where(np.any(img_np < 255, axis=0))[0]\n",
    "                if cols.size == 0: continue\n",
    "                trimmed = img_np[:, cols[0]:]\n",
    "                padded = pad_to_height(trimmed)\n",
    "                label = int(cls) if cls.isdigit() else BLANK_CLASS\n",
    "                dataset.append((padded, label))\n",
    "    return dataset\n",
    "\n",
    "def find_similar_edge(target_col, candidates, threshold=10):\n",
    "    random.shuffle(candidates)\n",
    "    for candidate_img, candidate_label, idx in candidates:\n",
    "        diff = np.mean(np.abs(candidate_img[:, 0] - target_col))\n",
    "        if diff < threshold:\n",
    "            return candidate_img, candidate_label, idx\n",
    "    return random.choice(candidates)\n",
    "\n",
    "def build_realistic_strip(digit_bank, none_bank, left_pad=LEFT_PAD, height=WINDOW_HEIGHT):\n",
    "    target_width = random.randint(60, MAX_WIDTH)\n",
    "    max_digits = 8\n",
    "    digit_count = 0\n",
    "\n",
    "    strip = np.full((height, left_pad), 255, dtype=np.uint8)\n",
    "    labels = []\n",
    "    current_width = left_pad\n",
    "\n",
    "    banks_available = [bank for bank in [digit_bank, none_bank] if bank]\n",
    "    current_bank = random.choice(banks_available)\n",
    "\n",
    "    # Pop the first image\n",
    "    prev_img, prev_label, prev_idx = current_bank.pop()\n",
    "    strip = np.concatenate((strip, prev_img), axis=1)\n",
    "    start = current_width\n",
    "    current_width += prev_img.shape[1]\n",
    "    if prev_label != BLANK_CLASS:\n",
    "        labels.append((prev_label, start, current_width))\n",
    "        digit_count += 1\n",
    "\n",
    "    while current_width < target_width and (digit_bank or none_bank):\n",
    "        # Determine which bank to sample from\n",
    "        banks_available = []\n",
    "        if digit_count < max_digits and digit_bank:\n",
    "            banks_available.append(digit_bank)\n",
    "        if none_bank:\n",
    "            banks_available.append(none_bank)\n",
    "        if not banks_available:\n",
    "            break\n",
    "\n",
    "        current_bank = random.choice(banks_available)\n",
    "        next_img, next_label, next_idx = find_similar_edge(prev_img[:, -1], current_bank)\n",
    "\n",
    "        # If too wide, skip\n",
    "        if current_width + 1 + next_img.shape[1] > target_width:\n",
    "            break\n",
    "\n",
    "        # Add transition column\n",
    "        transition_col = np.median(np.stack([prev_img[:, -1], next_img[:, 0]]), axis=0).astype(np.uint8).reshape(-1, 1)\n",
    "        strip = np.concatenate((strip, transition_col), axis=1)\n",
    "        current_width += 1\n",
    "\n",
    "        # Append next digit\n",
    "        start = current_width\n",
    "        strip = np.concatenate((strip, next_img), axis=1)\n",
    "        current_width += next_img.shape[1]\n",
    "        if next_label != BLANK_CLASS:\n",
    "            labels.append((next_label, start, current_width))\n",
    "            digit_count += 1\n",
    "\n",
    "        # Remove used sample\n",
    "        current_bank[:] = [item for item in current_bank if item[2] != next_idx]\n",
    "        prev_img = next_img\n",
    "\n",
    "    # Pad right\n",
    "    strip = np.concatenate((strip, np.full((height, left_pad), 255, dtype=np.uint8)), axis=1)\n",
    "    return strip, labels\n",
    "\n",
    "def create_full_dataset(base_dir):\n",
    "    all_images = load_and_trim_digit_images(base_dir)\n",
    "\n",
    "    # Assign unique index\n",
    "    digit_bank = [(img, lbl, idx) for idx, (img, lbl) in enumerate(all_images) if lbl != BLANK_CLASS]\n",
    "    none_bank  = [(img, lbl, idx) for idx, (img, lbl) in enumerate(all_images) if lbl == BLANK_CLASS]\n",
    "\n",
    "    total_unique = len(digit_bank) + len(none_bank)\n",
    "    used_indices = set()\n",
    "    dataset = []\n",
    "\n",
    "    pbar = tqdm(total=total_unique, desc=f\"Creating {base_dir} dataset\")\n",
    "\n",
    "    while digit_bank or none_bank:\n",
    "        strip_img, labels = build_realistic_strip(digit_bank, none_bank)\n",
    "\n",
    "        # Collect used indices\n",
    "        current_used = {idx for _, _, idx in digit_bank + none_bank}\n",
    "        newly_used = set(range(total_unique)) - current_used - used_indices\n",
    "        used_indices.update(newly_used)\n",
    "        pbar.update(len(newly_used))\n",
    "\n",
    "        dataset.append((strip_img, labels))\n",
    "\n",
    "    pbar.close()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_strips(dataset, num_samples=5):\n",
    "    samples = random.sample(dataset, num_samples)\n",
    "    for idx, (img, labels) in enumerate(samples, 1):\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Sample #{idx}\")\n",
    "        ax = plt.gca()\n",
    "        for label, start, end in labels:\n",
    "            rect = plt.Rectangle((start, 0), end-start, img.shape[0], \n",
    "                                 edgecolor='red', facecolor='none', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text((start+end)/2, -1, str(label), color='blue', fontsize=12, ha='center', va='bottom')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "def export_labeled_images(dataset, output_dir, prefix=\"strip\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for idx, (img, labels) in enumerate(dataset):\n",
    "        fig, ax = plt.subplots(figsize=(12, 2))\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        for label, start, end in labels:\n",
    "            rect = plt.Rectangle((start, 0), end - start, img.shape[0],\n",
    "                                 edgecolor='red', facecolor='none', linewidth=1)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text((start + end) / 2, -1, str(label), color='blue',\n",
    "                    fontsize=10, ha='center', va='bottom')\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{prefix}_{idx:04d}.png\"), dpi=100)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data = create_full_dataset(TRAIN_DIR)\n",
    "    val_data = create_full_dataset(VALI_DIR)\n",
    "\n",
    "    with open(\"train_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(\"val_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_data, f)\n",
    "\n",
    "    print(f\"\\n‚úÖ Dataset creation complete.\")\n",
    "    print(f\"Train set strips: {len(train_data)}\")\n",
    "    print(f\"Validation set strips: {len(val_data)}\")\n",
    "    count_labels_across_strips(train_data)\n",
    "    count_labels_across_strips(val_data)\n",
    "    # üé® Export all labeled strips to folders for visual inspection\n",
    "    #export_labeled_images(train_data, \"labeled_strips_train\", prefix=\"train\")\n",
    "    #export_labeled_images(val_data, \"labeled_strips_val\", prefix=\"val\")\n",
    "    # Show 5 random samples from training dataset\n",
    "    print(\"\\nüé® Showing 5 random training samples:\")\n",
    "    visualize_strips(train_data, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a52e4",
   "metadata": {},
   "source": [
    "## Part 2: Train CNN on INT8 Inputs + Calibrate Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd786f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# ====== Parameters ======\n",
    "K_size = 64\n",
    "NUM_CLASSES = 11\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 1000\n",
    "INPUT_WIDTH = 14\n",
    "INPUT_HEIGHT = 18\n",
    "FC_WIDTH = 12\n",
    "FC_HEIGHT = 16\n",
    "BLANK_CLASS = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üìü Using device: {device}\")\n",
    "\n",
    "# ====== Model ======\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, k=K_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, k, kernel_size=3, stride=1, padding=0)  # VALID\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(k * FC_HEIGHT * FC_WIDTH, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "       # x = x[:, :, 1:1+FC_HEIGHT, 1:1+FC_WIDTH]  # Crop to 16x12\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        return self.fc1(x)\n",
    "\n",
    "# ====== Dataset Loader =======\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, pkl_path, input_width=14, input_height=18, fc_width=12, fc_height=16,\n",
    "                 blank_class=10, pos_ratio=0.0, neg_ratio=0.5, limit_negatives=False):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "        self.samples = []\n",
    "        self.negatives = []\n",
    "        self.class_counts = defaultdict(int)\n",
    "\n",
    "        for img, labels in self.data:\n",
    "            h, w = img.shape\n",
    "            buffer = np.full((input_height, input_width), 255, dtype=np.uint8)\n",
    "            x = 0\n",
    "            active_label = None\n",
    "            reset_buffer = False\n",
    "            label_idx = 0\n",
    "            labels = sorted(labels, key=lambda x: x[1])  # Sort by start column\n",
    "\n",
    "            while x < w:\n",
    "                # Shift window left, insert new column\n",
    "                buffer[:, :-1] = buffer[:, 1:]\n",
    "                buffer[:, -1] = img[:, x]\n",
    "                x += 1\n",
    "\n",
    "                # Check for start of a new digit\n",
    "                if label_idx < len(labels):\n",
    "                    label, start, end = labels[label_idx]\n",
    "\n",
    "                    if start <= x - 1 <= end:  # inside digit\n",
    "                        active_label = (label, start, end)\n",
    "\n",
    "                    if active_label and x == end + 1:  # one column after end\n",
    "                        crop = np.clip(buffer.astype(np.int16) - 128, -128, 127).astype(np.int8)\n",
    "                        self.samples.append((torch.tensor(crop, dtype=torch.int8).unsqueeze(0), label))\n",
    "                        self.class_counts[label] += 1\n",
    "\n",
    "                        # Reset buffer but preserve last two columns\n",
    "                        last_two = buffer[:, -2:].copy()\n",
    "                        buffer.fill(255)\n",
    "                        buffer[:, -2:] = last_two\n",
    "\n",
    "                        active_label = None\n",
    "                        label_idx += 1\n",
    "                        continue  # skip overlap logic on reset\n",
    "\n",
    "                # If not in active digit ‚Äî treat as candidate negative\n",
    "                if not active_label:\n",
    "                    overlaps = [max(0, min(x, e) - max(x - input_width, s)) / (e - s)\n",
    "                                for _, s, e in labels]\n",
    "                    max_overlap = max(overlaps + [0])\n",
    "                    if max_overlap <= neg_ratio:\n",
    "                        crop = np.clip(buffer.astype(np.int16) - 128, -128, 127).astype(np.int8)\n",
    "                        self.negatives.append((torch.tensor(crop, dtype=torch.int8).unsqueeze(0), blank_class))\n",
    "\n",
    "        # Optionally filter negatives\n",
    "        if limit_negatives and self.negatives:\n",
    "            total_positives = len(self.samples)\n",
    "            max_negatives = total_positives\n",
    "            def score_negative_patch(img_tensor):\n",
    "                img = img_tensor.numpy()[0]\n",
    "                non_white_cols = np.sum(np.any(img != 127, axis=0))\n",
    "                return non_white_cols\n",
    "            scored = sorted(self.negatives, key=lambda x: -score_negative_patch(x[0]))\n",
    "            selected = scored[:max_negatives]\n",
    "            self.samples.extend(selected)\n",
    "            self.class_counts[blank_class] = len(selected)\n",
    "        else:\n",
    "            self.samples.extend(self.negatives)\n",
    "            self.class_counts[blank_class] = len(self.negatives)\n",
    "\n",
    "        total = sum(self.class_counts.values())\n",
    "        print(\"\\nüìä Class Distribution:\")\n",
    "        for k in sorted(self.class_counts):\n",
    "            name = str(k) if k != blank_class else \"blank\"\n",
    "            print(f\"  Class {name}: {self.class_counts[k]} ({(self.class_counts[k] / total)*100:.2f}%)\")\n",
    "        print(f\"  Total samples: {total}\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.samples[idx]\n",
    "        return img.to(torch.float32), label\n",
    "\n",
    "\n",
    "# ====== Training Function ======\n",
    "def train_model(model, train_loader, val_loader,l1_lambda =1e-5,l2_lambda = 1e-1,label_smoothing=0.2):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3,weight_decay=l2_lambda)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            # üî• Add L1 penalty\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        preds, labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "                val_loss += loss.item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.size(0)\n",
    "                preds.extend(pred.cpu().numpy())\n",
    "                labels.extend(y.cpu().numpy())\n",
    "\n",
    "        val_acc = 100 * correct / total\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, Val Acc={val_acc:.2f}%, \"\n",
    "              f\"Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, preds, labels\n",
    "\n",
    "# ====== Manual Quantization & Evaluation ======\n",
    "# ‚úÖ Improved Quantization Function\n",
    "def quantize_tensor(tensor, is_bias=False, is_fc_layer=False, k=K_size, percentile_clip=0.1):\n",
    "    if is_bias:\n",
    "        if is_fc_layer:\n",
    "            # Scaled FC bias (we keep as is)\n",
    "            scale_factor = (k * 16 * 12) * 100\n",
    "            return torch.clamp(torch.round(tensor * scale_factor), -2**31, 2**31 - 1).to(torch.int64)\n",
    "        else:\n",
    "            # Conv bias: symmetric absmax\n",
    "            scale = max(abs(tensor.min()), abs(tensor.max())) / 127 if tensor.max() != tensor.min() else 1.0\n",
    "            return torch.clamp(torch.round(tensor / scale), -2**15, 2**15 - 1).to(torch.int32)\n",
    "\n",
    "    # üéØ Improved FC Weight Quantization using symmetric percentile clipping\n",
    "    if is_fc_layer:\n",
    "        t_np = tensor.detach().cpu().numpy()\n",
    "        lower = np.percentile(t_np, percentile_clip)\n",
    "        upper = np.percentile(t_np, 100 - percentile_clip)\n",
    "        absmax = max(abs(lower), abs(upper))\n",
    "\n",
    "        if absmax == 0:\n",
    "            return torch.zeros_like(tensor, dtype=torch.int8)\n",
    "\n",
    "        scale = absmax / 127\n",
    "        tensor_clipped = torch.clamp(tensor, -absmax, absmax)\n",
    "        return torch.clamp(torch.round(tensor_clipped / scale), -128, 127).to(torch.int8)\n",
    "\n",
    "    # ‚úÖ Default convolution quantization (no change)\n",
    "    scale = max(abs(tensor.min()), abs(tensor.max())) / 127 if tensor.max() != tensor.min() else 1.0\n",
    "    return torch.clamp(torch.round(tensor / scale), -128, 127).to(torch.int8)\n",
    "\n",
    "\n",
    "def compute_thresholds(model, loader, weight=None, bias=None, percentile=5, blank_class=10):\n",
    "    model.eval()\n",
    "    per_class_scores = {i: [] for i in range(NUM_CLASSES)}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = model.relu1(model.conv1(x))\n",
    "            fc_input = out.reshape(out.size(0), -1).cpu().numpy().astype(np.int32)\n",
    "\n",
    "            if weight is None or bias is None:\n",
    "                weight = model.fc1.weight.detach().cpu().numpy().astype(np.int8)\n",
    "                bias = model.fc1.bias.detach().cpu().numpy().astype(np.int64)\n",
    "\n",
    "            result = fc_input @ weight.T.astype(np.int32) + bias  # Quantized forward\n",
    "\n",
    "            for score_vec, label in zip(result, y.numpy()):\n",
    "                per_class_scores[label].append(score_vec[label])  # True class score only\n",
    "\n",
    "    # Build per-class threshold dictionary\n",
    "    thresholds = {}\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        if cls == blank_class:\n",
    "            thresholds[cls] = 0  # ‚úÖ Always use 0 for blank class\n",
    "        else:\n",
    "            scores = per_class_scores[cls]\n",
    "            if len(scores) < 5:\n",
    "                thresholds[cls] = 0\n",
    "            else:\n",
    "                thresholds[cls] = int(np.percentile(scores, percentile))\n",
    "\n",
    "    print(f\"\\nüìè Per-Class Quantized Thresholds (percentile={percentile}):\")\n",
    "    for c in range(NUM_CLASSES):\n",
    "        print(f\"  Class {c}: {thresholds[c]}\")\n",
    "    return thresholds\n",
    "\n",
    "def compute_best_thresholds(model, loader, weight=None, bias=None, blank_class=10):\n",
    "\n",
    "    model.eval()\n",
    "    all_scores = []  # Each item: (score, pred_class, is_correct)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = model.relu1(model.conv1(x))\n",
    "            fc_input = out.reshape(out.size(0), -1).cpu().numpy().astype(np.int32)\n",
    "\n",
    "            if weight is None or bias is None:\n",
    "                weight = model.fc1.weight.detach().cpu().numpy().astype(np.int8)\n",
    "                bias = model.fc1.bias.detach().cpu().numpy().astype(np.int64)\n",
    "\n",
    "            result = fc_input @ weight.T.astype(np.int32) + bias\n",
    "            y_true = y.numpy()\n",
    "\n",
    "            for scores, true in zip(result, y_true):\n",
    "                for cls in range(NUM_CLASSES):\n",
    "                    score = scores[cls]\n",
    "                    is_correct = int(cls == true)\n",
    "                    all_scores.append((cls, score, is_correct))\n",
    "\n",
    "    # Now: group scores by class\n",
    "    thresholds = {}\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        plot_score_distribution(all_scores, cls)\n",
    "        if cls == blank_class:\n",
    "            thresholds[cls] = 0\n",
    "            continue\n",
    "\n",
    "        cls_scores = [(score, correct) for c, score, correct in all_scores if c == cls]\n",
    "        if len(cls_scores) < 10:\n",
    "            thresholds[cls] = 0\n",
    "            continue\n",
    "\n",
    "        scores, labels = zip(*cls_scores)\n",
    "        scores = np.array(scores)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Sweep thresholds and compute F1\n",
    "        best_thresh, best_f1 = 0, 0\n",
    "        for t in np.unique(scores):\n",
    "            preds = (scores >= t).astype(int)\n",
    "            f1 = f1_score(labels, preds, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = t\n",
    "\n",
    "        thresholds[cls] = int(best_thresh)\n",
    "\n",
    "    print(\"\\nüìè Calibrated Per-Class Thresholds (F1-optimized):\")\n",
    "    for c in range(NUM_CLASSES):\n",
    "        print(f\"  Class {c}: {thresholds[c]}\")\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def plot_score_distribution(all_scores, cls_id):\n",
    "    cls_scores = [score for c, score, correct in all_scores if c == cls_id]\n",
    "    cls_labels = [correct for c, score, correct in all_scores if c == cls_id]\n",
    "\n",
    "    pos_scores = [s for s, l in zip(cls_scores, cls_labels) if l == 1]\n",
    "    neg_scores = [s for s, l in zip(cls_scores, cls_labels) if l == 0]\n",
    "\n",
    "    plt.hist(pos_scores, bins=50, alpha=0.6, label=\"True Positives\")\n",
    "    plt.hist(neg_scores, bins=50, alpha=0.6, label=\"False Positives\")\n",
    "    plt.title(f\"Class {cls_id} Score Distribution\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_with_metrics(model, loader, thresholds, blank_class=10):\n",
    "    model.eval()\n",
    "    preds, labels = [],[]\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            out = out.cpu().numpy()\n",
    "            y = y.cpu().numpy()\n",
    "\n",
    "            for score, true_label in zip(out, y):\n",
    "                filtered = [s if s >= thresholds[i] else -np.inf for i, s in enumerate(score)]\n",
    "                pred = np.argmax(filtered) if np.any(np.isfinite(filtered)) else blank_class\n",
    "\n",
    "                preds.append(pred)\n",
    "                labels.append(true_label)\n",
    "\n",
    "                # Accuracy (exclude true blank==blank cases)\n",
    "                if not (true_label == blank_class and pred == blank_class):\n",
    "                    total += 1\n",
    "                    if pred == true_label:\n",
    "                        correct += 1\n",
    "\n",
    "    acc = 100 * correct / total if total > 0 else 0\n",
    "    print(f\"\\nüìè Adjusted Accuracy (excluding true blanks): {acc:.2f}%\")\n",
    "\n",
    "    # --- Digit-only accuracy ---\n",
    "    digit_preds = [p for p, t in zip(preds, labels) if t != blank_class]\n",
    "    digit_labels = [t for t in labels if t != blank_class]\n",
    "    digit_correct = sum(int(p == t) for p, t in zip(digit_preds, digit_labels))\n",
    "    digit_total = len(digit_labels)\n",
    "    digit_acc = 100 * digit_correct / digit_total if digit_total else 0\n",
    "    print(f\"üî¢ Digit-Only Accuracy: {digit_acc:.2f}%\")\n",
    "\n",
    "    # --- Precision / Recall / F1 ---\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, preds, labels=[*range(10), blank_class], zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìä Precision / Recall / F1 per class:\")\n",
    "    for cls in range(10):\n",
    "        print(f\"  Class {cls}: P={precision[cls]:.2f}, R={recall[cls]:.2f}, F1={f1[cls]:.2f}, Support={support[cls]}\")\n",
    "    print(f\"  Blank:    P={precision[blank_class]:.2f}, R={recall[blank_class]:.2f}, F1={f1[blank_class]:.2f}, Support={support[blank_class]}\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "\n",
    "def save_thresholds(thresholds, filename=\"thresholds.json\"):\n",
    "    data = {f\"class_{i}\": int(thr) for i, thr in enumerate(thresholds)}\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"‚úÖ Thresholds saved to {filename}\")\n",
    "\n",
    "\n",
    "def visualize_samples(dataset, title):\n",
    "    class_buckets = {i: [] for i in range(11)}  # 0-9 + blank (10)\n",
    "    for img, label in dataset:\n",
    "        if len(class_buckets[label]) < 10:\n",
    "            class_buckets[label].append(img)\n",
    "        if all(len(v) == 10 for v in class_buckets.values()):\n",
    "            break\n",
    "\n",
    "    fig, axes = plt.subplots(11, 10, figsize=(15, 15))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for class_id in range(11):\n",
    "        for i, sample in enumerate(class_buckets[class_id]):\n",
    "            axes[class_id, i].imshow(sample[0].numpy(), cmap='gray', vmin=-128, vmax=127)\n",
    "            axes[class_id, i].axis('off')\n",
    "        for j in range(len(class_buckets[class_id]), 10):\n",
    "            axes[class_id, j].axis('off')\n",
    "        axes[class_id, 0].set_ylabel(f\"{class_id if class_id < 10 else 'blank'}\", fontsize=12, rotation=0, labelpad=20)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,class_names, title):\n",
    "    num_classes = len(class_names)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "# ====== Main ======\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    train_set = SlidingWindowDataset(\"train_dataset.pkl\",pos_ratio=0.0, neg_ratio=0.0,limit_negatives=False)\n",
    "    val_set = SlidingWindowDataset(\"val_dataset.pkl\",pos_ratio=0.0, neg_ratio=0.0,limit_negatives=False)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=32)\n",
    "    class_names = [str(i) for i in range(10)] + [\"blank\"]\n",
    "\n",
    "    visualize_samples(train_set, \"üéì Train Set Samples per Class\")\n",
    "    visualize_samples(val_set, \"üß™ Validation Set Samples per Class\")\n",
    "\n",
    "    model = CNNModel().to(device)\n",
    "    model, preds, labels = train_model(model, train_loader, val_loader,l1_lambda=5e-5, l2_lambda=1e-2,label_smoothing=0.18)\n",
    "\n",
    "\n",
    "    # Compute and plot Confusion Matrix Before Quantization\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
    "            outputs = model(images)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # Use argmax to select highest FC score\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    plot_confusion_matrix(all_labels, all_preds, class_names,\"Validation Confusion Matrix Before Quantization\")\n",
    "\n",
    "    quantized_state_dict = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        is_fc_layer = \"fc1.bias\" in name  # ‚úÖ Detect if this is an FC bias\n",
    "        is_bias = \"bias\" in name  # ‚úÖ Detect if this is a bias\n",
    "        k = model.conv1.out_channels  # ‚úÖ Get k value from Conv2D\n",
    "\n",
    "        # ‚úÖ Keep everything the same, only adjust FC bias scaling\n",
    "        quantized_state_dict[name] = quantize_tensor(param, is_bias=is_bias, is_fc_layer=is_fc_layer, k=k)\n",
    "\n",
    "    # ‚úÖ Save Quantized Model\n",
    "    torch.save(quantized_state_dict, \"quantized_model.pth\") \n",
    "    print(\"‚úÖ Quantized model saved successfully with fixed FC bias scaling!\")\n",
    "\n",
    "\n",
    "    model.load_state_dict(quantized_state_dict, strict=False)\n",
    "\n",
    "    # Post-Quantization Validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
    "            outputs = model(images)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)  # Use argmax to select highest FC score\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "    plot_confusion_matrix(all_labels, all_preds, class_names,\"Validation Confusion Matrix After Quantization without thresholds\")\n",
    "    thresholds = compute_best_thresholds(model, val_loader)\n",
    "    q_preds, q_labels = evaluate_with_metrics(model, val_loader, thresholds)\n",
    "    plot_confusion_matrix(q_labels,q_preds, class_names, \"Validation Confusion Matrix After Quantization\")\n",
    "\n",
    "    save_thresholds(thresholds, \"thresholds.json\")\n",
    "    print(\"‚úÖ Per-Class Thresholds:\")\n",
    "    for cls, t in thresholds.items():\n",
    "        print(f\"  Class {cls}: {t}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f3011",
   "metadata": {},
   "source": [
    "## Part 3: Export .mif for FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eacee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Binary & Hex Conversion =========\n",
    "def int_to_bin(value, bit_width):\n",
    "    if value < 0:\n",
    "        value = (1 << bit_width) + value\n",
    "    return format(value, f'0{bit_width}b')\n",
    "\n",
    "def int_to_hex(value, bit_width):\n",
    "    if value < 0:\n",
    "        value = (1 << bit_width) + value\n",
    "    hex_digits = bit_width // 4\n",
    "    return format(value, f'0{hex_digits}X')\n",
    "\n",
    "# ========= Save MIF File =========\n",
    "def save_mif(filename, data, word_size, values_per_line, format_type=\"HEX\"):\n",
    "    depth = data.shape[0]\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"WIDTH={word_size * values_per_line};\\n\")\n",
    "        f.write(f\"DEPTH={depth};\\n\")\n",
    "        f.write(\"ADDRESS_RADIX=HEX;\\n\")\n",
    "        f.write(f\"DATA_RADIX={format_type};\\n\")\n",
    "        f.write(\"CONTENT BEGIN\\n\")\n",
    "        for i in range(depth):\n",
    "            if format_type == \"BIN\":\n",
    "                values = \"\".join(int_to_bin(int(val), word_size) for val in data[i])\n",
    "            else:\n",
    "                values = \"\".join(int_to_hex(int(val), word_size) for val in data[i])\n",
    "            f.write(f\"{i:X} : {values};\\n\")\n",
    "        f.write(\"END;\\n\")\n",
    "\n",
    "# ========= Export Conv2D Weights =========\n",
    "def export_conv2d_weights_to_mif(model, filename=\"CON_W.mif\"):\n",
    "    weights = model.state_dict()[\"conv1.weight\"].numpy().squeeze(1)  # Shape: (K, 3, 3)\n",
    "\n",
    "    # Flatten each filter in row-major order (W00 to W22)\n",
    "    flat_filters = [w.flatten().astype(np.int8) for w in weights]\n",
    "\n",
    "    lines = []\n",
    "    for i in range(0, len(flat_filters), 2):\n",
    "        f1 = flat_filters[i]\n",
    "        f2 = flat_filters[i + 1] if i + 1 < len(flat_filters) else np.zeros(9, dtype=np.int8)\n",
    "        combined = np.concatenate((f1, f2))  # 18 values per line\n",
    "        lines.append(combined)\n",
    "\n",
    "    data = np.array(lines, dtype=np.int8)  # Convert list of arrays to 2D NumPy array\n",
    "    save_mif(filename, data, word_size=8, values_per_line=18, format_type=\"HEX\")\n",
    "    print(f\"‚úÖ Conv2D weights exported to '{filename}' as HEX\")\n",
    "\n",
    "# ========= Export Biases =========\n",
    "def export_biases_to_mif(model):\n",
    "    conv_bias = model.state_dict()[\"conv1.bias\"].numpy().astype(np.int16)\n",
    "    conv_bias = conv_bias.reshape(-1, 2)\n",
    "    save_mif(\"CON_B.mif\", conv_bias, word_size=16, values_per_line=2, format_type=\"HEX\")\n",
    "\n",
    "    fc_bias = model.state_dict()[\"fc1.bias\"].numpy().astype(np.int64).reshape(-1, 1)\n",
    "    save_mif(\"FCM_B.mif\", fc_bias, word_size=45, values_per_line=1, format_type=\"BIN\")\n",
    "    print(\"‚úÖ Conv2D & FC biases saved.\")\n",
    "\n",
    "# ========= Export FC Weights =========\n",
    "def export_fc_weights_to_mif(model):\n",
    "    weights = model.state_dict()[\"fc1.weight\"].numpy().astype(np.int8).flatten()\n",
    "    padded_len = int(np.ceil(len(weights) / 128)) * 128\n",
    "    weights = np.pad(weights, (0, padded_len - len(weights)), 'constant')\n",
    "    weights = weights.reshape(-1, 128)\n",
    "    save_mif(\"FCM_W.mif\", weights, word_size=8, values_per_line=128, format_type=\"HEX\")\n",
    "    print(\"‚úÖ FC weights exported to FCM_W.mif\")\n",
    "\n",
    "# ========= Export ReLU Default Output =========\n",
    "def export_relu_default_output_dynamic(model):\n",
    "    input_image = torch.full((1, 1, 18, 14), 127, dtype=torch.float32)\n",
    "    conv_output = model.conv1(input_image)\n",
    "    relu_output = torch.relu(conv_output).detach().numpy().astype(np.int32)\n",
    "\n",
    "    k = conv_output.shape[1]\n",
    "    assert k % 2 == 0, \"Number of filters (k) must be even.\"\n",
    "\n",
    "    word_size = 19\n",
    "    values_per_line = 384  # 2 filters' worth\n",
    "    total_width = word_size * values_per_line\n",
    "    num_lines = k // 2\n",
    "\n",
    "    with open(\"REL_O.mif\", \"w\") as f:\n",
    "        f.write(f\"WIDTH={total_width};\\n\")\n",
    "        f.write(f\"DEPTH={num_lines};\\n\")\n",
    "        f.write(\"ADDRESS_RADIX=HEX;\\n\")\n",
    "        f.write(\"DATA_RADIX=BIN;\\n\")\n",
    "        f.write(\"CONTENT BEGIN\\n\")\n",
    "\n",
    "        for i in range(0, k, 2):\n",
    "            filt_pair = relu_output[0, i:i+2, :, :]  # (2, 16, 12)\n",
    "            values = filt_pair.flatten()\n",
    "            bin_line = \"\".join(int_to_bin(v, word_size) for v in values)\n",
    "            f.write(f\"{i//2:X} : {bin_line};\\n\")\n",
    "\n",
    "        f.write(\"END;\\n\")\n",
    "\n",
    "    print(f\"‚úÖ REL_O.mif exported successfully with WIDTH={total_width}, DEPTH={num_lines}, Filters={k}\")\n",
    "\n",
    "# ========= Execute All =========\n",
    "model = CNNModel(num_classes=11, k=K_size)\n",
    "model.load_state_dict(torch.load(\"quantized_model.pth\", map_location=\"cpu\"))\n",
    "\n",
    "export_conv2d_weights_to_mif(model)\n",
    "export_biases_to_mif(model)\n",
    "export_fc_weights_to_mif(model)\n",
    "export_relu_default_output_dynamic(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190cb4f",
   "metadata": {},
   "source": [
    "## Part 4: Test and export result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afd9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def emulate_fpga_sliding_detection(\n",
    "    image, model, thresholds,\n",
    "    input_width=14, input_height=18, keep_cols=2,\n",
    "    fc_height=16, fc_width=12,\n",
    "    blank_class=10, debug=False,\n",
    "    save_fc=False, save_dir=\"fc_debug\",\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if save_fc:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    h, w = image.shape\n",
    "    pad_top = (input_height - h) // 2\n",
    "    pad_bottom = input_height - h - pad_top\n",
    "    padded_img = np.pad(image, ((pad_top, pad_bottom), (0, 0)), constant_values=255)\n",
    "\n",
    "    buffer = np.full((input_height, input_width), 255, dtype=np.uint8)\n",
    "    x = 0\n",
    "    detections = []\n",
    "    step = 0\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    if debug:\n",
    "        debug_windows = []\n",
    "        debug_scores = []\n",
    "\n",
    "    while x < padded_img.shape[1]:\n",
    "        # Shift buffer\n",
    "        buffer[:, :-1] = buffer[:, 1:]\n",
    "        buffer[:, -1] = padded_img[:, x]\n",
    "        x += 1\n",
    "\n",
    "        crop = np.clip(buffer.astype(np.int16) - 128, -128, 127).astype(np.int8)\n",
    "        window_tensor = torch.tensor(crop, dtype=torch.int8).unsqueeze(0).unsqueeze(0).to(torch.float32).to(device)\n",
    "\n",
    "        # Get conv+ReLU output (also used for FC visualization)\n",
    "        with torch.no_grad():\n",
    "            relu_out = model.relu1(model.conv1(window_tensor))  # [1, K, H, W]\n",
    "            fc_input = relu_out[:, :, :fc_height, :fc_width]    # [1, K, 16, 12]\n",
    "            logits = model.fc1(fc_input.reshape(1, -1)).squeeze(0).cpu().numpy()\n",
    "\n",
    "        predicted_class = np.argmax(logits)\n",
    "        threshold = thresholds.get(predicted_class, 0)\n",
    "\n",
    "        # Save FC feature maps if enabled\n",
    "        if save_fc:\n",
    "            fc_array = fc_input.squeeze(0).cpu().numpy()  # [K, 16, 12]\n",
    "            \n",
    "\n",
    "            with open(os.path.join(save_dir, f\"fc_step_{step:03d}.txt\"), \"w\") as f:\n",
    "                for k, fmap in enumerate(fc_array):\n",
    "                    f.write(f\"Filter {k}:\\n\")\n",
    "                    for row in fmap:\n",
    "                        f.write(\" \".join(f\"{v:5.1f}\" for v in row) + \"\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "        # Debug storage\n",
    "        if debug:\n",
    "            debug_windows.append(buffer.copy())\n",
    "            debug_scores.append(logits.copy())\n",
    "\n",
    "        # Detection condition (do not reset for blank!)\n",
    "        if predicted_class != blank_class and logits[predicted_class] >= threshold:\n",
    "            detections.append((predicted_class, x - input_width, x))\n",
    "            last_two = buffer[:, -keep_cols:].copy()\n",
    "            buffer.fill(255)\n",
    "            buffer[:, -keep_cols:] = last_two\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # Debug visualization\n",
    "    if debug and debug_windows:\n",
    "        num_windows = len(debug_windows)\n",
    "        fig, axes = plt.subplots(num_windows, 2, figsize=(10, num_windows * 2))\n",
    "        if num_windows == 1:\n",
    "            axes = [axes]\n",
    "        for i, (win, scores) in enumerate(zip(debug_windows, debug_scores)):\n",
    "            ax_img, ax_bar = axes[i]\n",
    "            ax_img.imshow(win, cmap='gray', vmin=0, vmax=255)\n",
    "            ax_img.axis('off')\n",
    "            ax_img.set_title(f\"Step {i}\")\n",
    "\n",
    "            bars = ax_bar.bar(range(len(scores)), scores)\n",
    "            ax_bar.set_ylim(min(scores) - 1, max(scores) + 1)\n",
    "            ax_bar.set_title(\"Scores\")\n",
    "            for bar, score in zip(bars, scores):\n",
    "                ax_bar.text(bar.get_x() + bar.get_width() / 2.0, bar.get_height(),\n",
    "                            f\"{score:.0f}\", ha='center', va='bottom', fontsize=8, rotation=90)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return detections\n",
    "\n",
    "\n",
    "\n",
    "# Example usage (do not run in this snippet if not desired):\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose 'my_image' is a numpy array (grayscale) with shape (H, W)\n",
    "    # and 'model' is your CNNModel already loaded on the correct device.\n",
    "    # 'thresholds' is a dictionary mapping class index to threshold value, e.g.:\n",
    "    thresholds  # simple initial thresholds; update as needed\n",
    "    # For blank class, thresholds[BLANK_CLASS] remains 0.\n",
    "\n",
    "    # For example, if you load an image:\n",
    "    my_image = plt.imread(\"643_object_0.png\")\n",
    "    if my_image.max() <= 1.0:\n",
    "        my_image = (my_image * 255).astype(np.uint8)\n",
    "\n",
    "    # (Ensure my_image is a 2D array of type np.uint8)\n",
    "    #\n",
    "    # Then:\n",
    "    model = model.to(device)  # Ensure model is on the correct device\n",
    "    \n",
    "    model.load_state_dict(torch.load(\"quantized_model.pth\"))\n",
    "    model.eval()\n",
    "    model.to(device)  # Move model to GPU if using CUDA\n",
    "\n",
    "    detections = emulate_fpga_sliding_detection(\n",
    "        image=my_image,\n",
    "        model=model,\n",
    "        thresholds=thresholds,\n",
    "        debug=True,\n",
    "        save_fc=True,\n",
    "        save_dir=\"fc_debug\",\n",
    "        device=device\n",
    "    )   \n",
    "    #\n",
    "    # You can then process 'detections' as needed.\n",
    "    print(f\"{detections}\")\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LPR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
